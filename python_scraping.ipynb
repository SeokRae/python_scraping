{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indeed Job Page Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 50\n",
    "# indeed url\n",
    "Q = \"python\"\n",
    "INDEED_URL = f\"https://kr.indeed.com/jobs?q={Q}&limit={LIMIT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test indeed_page\n",
    "def get_indeed_last_page():\n",
    "    get_request = requests.get(INDEED_URL)\n",
    "    html_parse = bs(get_request.text, 'html.parser') # HTML 파싱\n",
    "\n",
    "    # pagination > a > span 찾기\n",
    "    pages = html_parse.find(\"div\", {\"class\": \"pagination\"}).find_all('a')\n",
    "\n",
    "    page_list = []\n",
    "\n",
    "    # 마지막 값(Next)을 제외한 page를 저장\n",
    "    for page in pages[:-1]:\n",
    "        page_value = int(page.find('span').string)\n",
    "        page_list.append(page_value)\n",
    "\n",
    "    max_page = page_list[-1]\n",
    "    \n",
    "    return max_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ineed_job(html):\n",
    "    # title\n",
    "    h2_title = html.find(\"h2\", {\"class\": \"title\"})\n",
    "    title = h2_title.find(\"a\")[\"title\"]\n",
    "    # company\n",
    "    span_company = html.find(\"span\", {\"class\": \"company\"})\n",
    "    a_company = span_company.find(\"a\")\n",
    "\n",
    "    company = \"\"\n",
    "    if a_company is not None:\n",
    "        company = str(a_company.string)\n",
    "    else:\n",
    "        company = str(span_company.string)\n",
    "    company = company.strip()\n",
    "\n",
    "    location = html.find(\"div\", {\"class\": \"recJobLoc\"})[\"data-rc-loc\"]\n",
    "    job_id = html[\"data-jk\"]\n",
    "    \n",
    "    return {\"title\": title, \"company\": company, \"location\": location, \"link\": f\"https://kr.indeed.com/viewjob?jk={job_id}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ineed_jobs(last_page):\n",
    "    jobs = []\n",
    "    for page in range(last_page):\n",
    "        print(f\" scrapping {page}\")\n",
    "    \n",
    "    # https://kr.indeed.com/jobs?q=python&limit=50&start=0\n",
    "    get_request = requests.get(f\"{INDEED_URL}&start={page*LIMIT}\")\n",
    "    \n",
    "    # print(f\" request status: {get_request.status_code}\")\n",
    "    html_parse = bs(get_request.text, 'html.parser') # HTML 파싱\n",
    "    results = html_parse.find_all(\"div\", {\"class\": \"jobsearch-SerpJobCard\"})\n",
    "    \n",
    "    for result in results:\n",
    "      job = extract_ineed_job(result)\n",
    "      jobs.append(job)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# indeed jobs\n",
    "def get_indeed_jobs():\n",
    "    last_page = get_indeed_last_page()\n",
    "    jobs = extract_ineed_jobs(last_page)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack of Flow Job Page Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indeed url\n",
    "STACK_OF_FLOW_URL = f\"https://stackoverflow.com/jobs?q={Q}&sort=i\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. page\n",
    "def get_stack_last_page():\n",
    "    # 2. requests\n",
    "    request = requests.get(STACK_OF_FLOW_URL)\n",
    "    html_parse = bs(request.text, \"html.parser\")\n",
    "    # pagination\n",
    "    pagination = html_parse.find(\"div\", {\"class\": \"s-pagination\"})\n",
    "    pages = pagination.find_all(\"a\")\n",
    "    # next 제거후 마지막 page\n",
    "    last_page = pages[-2].get_text(strip=True)\n",
    "    return int(last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. requests\n",
    "def extract_stack_job(html):\n",
    "    div = html.find(\"div\", {\"class\": \"grid--cell fl1\"})\n",
    "    # title  \n",
    "    h2_title = div.find(\"h2\", {\"class\": \"fs-body3\"})\n",
    "    title = h2_title.find(\"a\")[\"title\"]\n",
    "\n",
    "    if title is not None:\n",
    "        title = title\n",
    "    # company, location\n",
    "    company, location = div.find(\"h3\", {\"class\": \"fs-body1\"}).find_all(\"span\", recursive=False)\n",
    "    company = company.get_text(strip=True)\n",
    "    location = location.get_text(strip=True)\n",
    "\n",
    "    # link\n",
    "    job_id = html['data-jobid']\n",
    "\n",
    "    return {\n",
    "        \"title\": title, \n",
    "        \"company\": company, \n",
    "        \"location\": location,\n",
    "        \"link\": f\"https://stackoverflow.com/jobs/{job_id}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. job\n",
    "def extract_stack_jobs(last_page):\n",
    "    jobs = []\n",
    "    for page in range(last_page):\n",
    "        print(f\" scrapping stack_of_flow page: {page}\")\n",
    "        get_request = requests.get(f\"{STACK_OF_FLOW_URL}&pg={page + 1}\")\n",
    "        html_parse = bs(get_request.text, \"html.parser\")\n",
    "        results = html_parse.find_all(\"div\", {\"class\": \"-job\"})\n",
    "        for result in results:\n",
    "            job = extract_stack_job(result)\n",
    "            jobs.append(job)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main \n",
    "def get_stack_jobs():\n",
    "\n",
    "    last_page = get_stack_last_page()\n",
    "    jobs = extract_stack_jobs(last_page)\n",
    "\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(jobs):\n",
    "    file = open(\"jobs.csv\", mode=\"w\", encoding='utf-8', newline='')\n",
    "    write = csv.writer(file)\n",
    "    header = [\"title\", \"company\", \"location\", \"link\"]\n",
    "    write.writerow(header)\n",
    "    \n",
    "    for job in jobs:\n",
    "        write.writerow(list(job.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " scrapping 0\n",
      " scrapping 1\n",
      " scrapping 2\n",
      " scrapping 3\n",
      " scrapping 4\n",
      " scrapping 5\n",
      " scrapping 6\n",
      " scrapping 7\n",
      " scrapping 8\n",
      " scrapping 9\n",
      " scrapping 10\n",
      " scrapping 11\n",
      " scrapping 12\n",
      " scrapping stack_of_flow page: 0\n",
      " scrapping stack_of_flow page: 1\n",
      " scrapping stack_of_flow page: 2\n",
      " scrapping stack_of_flow page: 3\n",
      " scrapping stack_of_flow page: 4\n",
      " scrapping stack_of_flow page: 5\n",
      " scrapping stack_of_flow page: 6\n",
      " scrapping stack_of_flow page: 7\n",
      " scrapping stack_of_flow page: 8\n",
      " scrapping stack_of_flow page: 9\n",
      " scrapping stack_of_flow page: 10\n",
      " scrapping stack_of_flow page: 11\n",
      " scrapping stack_of_flow page: 12\n",
      " scrapping stack_of_flow page: 13\n",
      " scrapping stack_of_flow page: 14\n",
      " scrapping stack_of_flow page: 15\n",
      " scrapping stack_of_flow page: 16\n",
      " scrapping stack_of_flow page: 17\n",
      " scrapping stack_of_flow page: 18\n",
      " scrapping stack_of_flow page: 19\n",
      " scrapping stack_of_flow page: 20\n",
      " scrapping stack_of_flow page: 21\n",
      " scrapping stack_of_flow page: 22\n",
      " scrapping stack_of_flow page: 23\n",
      " scrapping stack_of_flow page: 24\n",
      " scrapping stack_of_flow page: 25\n",
      " scrapping stack_of_flow page: 26\n",
      " scrapping stack_of_flow page: 27\n",
      " scrapping stack_of_flow page: 28\n",
      " scrapping stack_of_flow page: 29\n",
      " scrapping stack_of_flow page: 30\n",
      " scrapping stack_of_flow page: 31\n",
      " scrapping stack_of_flow page: 32\n",
      " scrapping stack_of_flow page: 33\n",
      " scrapping stack_of_flow page: 34\n",
      " scrapping stack_of_flow page: 35\n",
      " scrapping stack_of_flow page: 36\n",
      " scrapping stack_of_flow page: 37\n",
      " scrapping stack_of_flow page: 38\n",
      " scrapping stack_of_flow page: 39\n",
      " scrapping stack_of_flow page: 40\n",
      " scrapping stack_of_flow page: 41\n",
      " scrapping stack_of_flow page: 42\n",
      " scrapping stack_of_flow page: 43\n",
      " scrapping stack_of_flow page: 44\n",
      " scrapping stack_of_flow page: 45\n",
      " scrapping stack_of_flow page: 46\n",
      " scrapping stack_of_flow page: 47\n",
      " scrapping stack_of_flow page: 48\n",
      " scrapping stack_of_flow page: 49\n",
      " scrapping stack_of_flow page: 50\n",
      " scrapping stack_of_flow page: 51\n",
      " scrapping stack_of_flow page: 52\n",
      " scrapping stack_of_flow page: 53\n",
      " scrapping stack_of_flow page: 54\n",
      " scrapping stack_of_flow page: 55\n",
      " scrapping stack_of_flow page: 56\n",
      " scrapping stack_of_flow page: 57\n",
      " scrapping stack_of_flow page: 58\n",
      " scrapping stack_of_flow page: 59\n",
      " scrapping stack_of_flow page: 60\n",
      " scrapping stack_of_flow page: 61\n",
      " scrapping stack_of_flow page: 62\n",
      " scrapping stack_of_flow page: 63\n",
      " scrapping stack_of_flow page: 64\n",
      " scrapping stack_of_flow page: 65\n",
      " scrapping stack_of_flow page: 66\n",
      " scrapping stack_of_flow page: 67\n",
      " scrapping stack_of_flow page: 68\n",
      " scrapping stack_of_flow page: 69\n",
      " scrapping stack_of_flow page: 70\n",
      " scrapping stack_of_flow page: 71\n",
      " scrapping stack_of_flow page: 72\n",
      " scrapping stack_of_flow page: 73\n",
      " scrapping stack_of_flow page: 74\n",
      " scrapping stack_of_flow page: 75\n",
      " scrapping stack_of_flow page: 76\n",
      " scrapping stack_of_flow page: 77\n",
      " scrapping stack_of_flow page: 78\n",
      " scrapping stack_of_flow page: 79\n",
      " scrapping stack_of_flow page: 80\n",
      " scrapping stack_of_flow page: 81\n",
      " scrapping stack_of_flow page: 82\n",
      " scrapping stack_of_flow page: 83\n",
      " scrapping stack_of_flow page: 84\n",
      " scrapping stack_of_flow page: 85\n",
      " scrapping stack_of_flow page: 86\n",
      " scrapping stack_of_flow page: 87\n",
      " scrapping stack_of_flow page: 88\n",
      " scrapping stack_of_flow page: 89\n",
      " scrapping stack_of_flow page: 90\n",
      " scrapping stack_of_flow page: 91\n",
      " scrapping stack_of_flow page: 92\n",
      " scrapping stack_of_flow page: 93\n",
      " scrapping stack_of_flow page: 94\n",
      " scrapping stack_of_flow page: 95\n",
      " scrapping stack_of_flow page: 96\n",
      " scrapping stack_of_flow page: 97\n",
      " scrapping stack_of_flow page: 98\n",
      " scrapping stack_of_flow page: 99\n",
      " scrapping stack_of_flow page: 100\n",
      " scrapping stack_of_flow page: 101\n",
      " scrapping stack_of_flow page: 102\n",
      " scrapping stack_of_flow page: 103\n",
      " scrapping stack_of_flow page: 104\n",
      " scrapping stack_of_flow page: 105\n",
      " scrapping stack_of_flow page: 106\n",
      " scrapping stack_of_flow page: 107\n",
      " scrapping stack_of_flow page: 108\n",
      " scrapping stack_of_flow page: 109\n",
      " scrapping stack_of_flow page: 110\n",
      " scrapping stack_of_flow page: 111\n",
      " scrapping stack_of_flow page: 112\n",
      " scrapping stack_of_flow page: 113\n",
      " scrapping stack_of_flow page: 114\n",
      " scrapping stack_of_flow page: 115\n",
      " scrapping stack_of_flow page: 116\n",
      " scrapping stack_of_flow page: 117\n",
      " scrapping stack_of_flow page: 118\n",
      " scrapping stack_of_flow page: 119\n",
      " scrapping stack_of_flow page: 120\n",
      " scrapping stack_of_flow page: 121\n",
      " scrapping stack_of_flow page: 122\n",
      " scrapping stack_of_flow page: 123\n",
      " scrapping stack_of_flow page: 124\n",
      " scrapping stack_of_flow page: 125\n",
      " scrapping stack_of_flow page: 126\n",
      " scrapping stack_of_flow page: 127\n",
      " scrapping stack_of_flow page: 128\n",
      " scrapping stack_of_flow page: 129\n",
      " scrapping stack_of_flow page: 130\n",
      " scrapping stack_of_flow page: 131\n",
      " scrapping stack_of_flow page: 132\n",
      " scrapping stack_of_flow page: 133\n",
      " scrapping stack_of_flow page: 134\n",
      " scrapping stack_of_flow page: 135\n",
      " scrapping stack_of_flow page: 136\n",
      " scrapping stack_of_flow page: 137\n",
      " scrapping stack_of_flow page: 138\n",
      " scrapping stack_of_flow page: 139\n",
      " scrapping stack_of_flow page: 140\n",
      " scrapping stack_of_flow page: 141\n",
      " scrapping stack_of_flow page: 142\n",
      " scrapping stack_of_flow page: 143\n",
      " scrapping stack_of_flow page: 144\n",
      " scrapping stack_of_flow page: 145\n",
      " scrapping stack_of_flow page: 146\n",
      " scrapping stack_of_flow page: 147\n",
      " scrapping stack_of_flow page: 148\n",
      " scrapping stack_of_flow page: 149\n",
      " scrapping stack_of_flow page: 150\n",
      " scrapping stack_of_flow page: 151\n",
      " scrapping stack_of_flow page: 152\n",
      " scrapping stack_of_flow page: 153\n",
      " scrapping stack_of_flow page: 154\n",
      " scrapping stack_of_flow page: 155\n",
      " scrapping stack_of_flow page: 156\n",
      " scrapping stack_of_flow page: 157\n",
      " scrapping stack_of_flow page: 158\n",
      " scrapping stack_of_flow page: 159\n",
      " scrapping stack_of_flow page: 160\n",
      " scrapping stack_of_flow page: 161\n",
      " scrapping stack_of_flow page: 162\n",
      " scrapping stack_of_flow page: 163\n",
      " scrapping stack_of_flow page: 164\n",
      "4124\n"
     ]
    }
   ],
   "source": [
    "indeed = get_indeed_jobs()\n",
    "stack = get_stack_jobs()\n",
    "\n",
    "all_jobs = indeed + stack\n",
    "print(f\" Scraping total data: {len(all_jobs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_file(all_jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
